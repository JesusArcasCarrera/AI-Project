{% extends "base_faceapi.html" %}

{% block body %}
    <style>
        body {
            margin: 0;
            padding: 0;
            width: 100vw;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        canvas {
            position: absolute;
        }

        #results {
            display: none;
            visibility: hidden;
        }

        header {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 10px;
        }

    </style>
    <video id="video" width="720" height="560" autoplay muted></video>
    <div id="results">
        <h1> Resultados extraídos </h1>
        <div id="resultados"></div>
    </div>

    <script>
        var model;
        var classification = {0: 'with_mask', 1: 'without_mask', 2: 'wrong_mask'}
        var predictedIndex;
        var colors = {0: '#FF1493', 1: '#FFA500', 2: '#32CD32'}
        const video = document.getElementById('video')
        Promise.all([
            faceapi.nets.ssdMobilenetv1.loadFromUri("{{ url_for('static', filename='models/') }}"),
            faceapi.nets.faceRecognitionNet.loadFromUri("{{ url_for('static', filename='models/') }}"),
        ]).then(startVideo)

        async function startVideo() {
            model = await tf.loadLayersModel("{{ url_for('static', filename='models/tfjs/model.json') }}")
            navigator.getUserMedia(
                {
                    video: {}
                },
                stream => video.srcObject = stream,
                err => console.error(err)
            )
        }

        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video)
            document.body.append(canvas)
            const displaySize = { width: video.width, height: video.height }
            faceapi.matchDimensions(canvas, displaySize)
            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(video, new faceapi.SsdMobilenetv1Options({minConfidence: 0.18, maxFaces: 20}))
                const resizedDetections = faceapi.resizeResults(detections, displaySize)

                if (resizedDetections.length > 0) {
                    resizedDetections.forEach(d =>
                    {
                        extractFaceFromBox(video, d._box)
                        const drawOptions = {
                            boxColor: colors[predictedIndex]
                        }               
                        canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
                        const drawBox = new faceapi.draw.DrawBox(d._box, drawOptions)
                        drawBox.draw(canvas)
                    })       
                } else {
                    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
                    //faceapi.draw.drawDetections(canvas, resizedDetections)
                }
            }, 100)
        })

        /* 
            Extrae las caras detectadas por Face-API y las anida en un elemento <div> del DOM
        */
        async function extractFaceFromBox(inputImage, box) {
            const regionsToExtract = [
                new faceapi.Rect(box.x, box.y, box.width, box.height)
            ]

            let faceImages = await faceapi.extractFaces(inputImage, regionsToExtract)

            if (faceImages.length == 0) {
                console.log('Face not found')
            }
            else {
                document.getElementById("resultados").innerHTML = "";
                faceImages.forEach(e => {
                    document.getElementById("resultados").appendChild(e);
                    // TODO: Limitar los fotogramas donde se realiza la predicción para evitar lag
                    result = model.predict(preprocess(e))
                    // console.log(result.dataSync())
                    let maxValue = result.dataSync().reduce((m, n) => Math.max(m, n))
                    predictedIndex = result.dataSync().indexOf(maxValue)
                    // Debugging stuff
                    // console.log(result.dataSync())
                    // console.log(result.dataSync().reduce((m, n) => Math.max(m, n)))
                    // console.log(predictedIndex)
                    // Label the face
                    document.getElementById("clasificacion").innerHTML = classification[predictedIndex]
                })
            }
        }

        /*
            (En esta versión del código no se está utilizando)
            Realiza el preprocesado de una imagen para el modelo entrenado, que al final lo deja con un shape de [1, 96, 96, 3]
        */
        function preprocess(img) {
            const example = tf.browser.fromPixels(img);
            example.print();
            const resized = tf.image.resizeBilinear(example, [96, 96]).toFloat()
            const offset = tf.scalar(255.0);
            const normalized = tf.scalar(1.0).sub(resized.div(offset));
            const batched = normalized.expandDims(0)
            return batched
        }
    </script>
{% endblock %}