{% extends "base_faceapi.html" %}

{% block body %}
<style>
    body {
        margin: 0;
        padding: 0;
        width: 100vw;
        height: 100vh;
        display: flex;
        justify-content: center;
        align-items: center;
    }

    canvas {
        position: absolute;
    }

    #outcome {
        display: none;
        visibility: hidden;
    }

    header {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100px;
    }

    #video {
        min-width: 720px;
        min-height: 560px;
    }


    /* loadbar */

    #container_modal {
        display: none;
        justify-content: center;
        align-items: center;
        position: absolute;
        left: 0;
        top: 0;
        z-index: 10;
        background-color: rgba(0, 0, 0, 0.3);
        width: 100%;
        height: 100%;
    }

    #container_loadbar {
        display: flex;
        justify-content: space-around;
        align-items: flex-start;
        background-color: white;
        z-index: 0;
        min-width: 50%;
        min-height: 10%;
        border-radius: .5rem;
        padding: 0.5rem;
    }

    #message {
        display: flex;
        justify-content: center;
        width: 100%;
    }

    #progress {
        width: 100%;
        height: 2rem;
        color: black;
        display: inline;
    }

    #progress-bar {
        border-color: black;
        width: 100%;
        height: 100%;
        top: auto;
        border-radius: .5rem;

    }

    .progress-bar.indeterminate {
        position: relative;
        animation: progress-indeterminate 4s linear infinite forwards;
    }

    @keyframes progress-indeterminate {
        0% {
            left: 0%;
            width: 0%;
        }

        5% {
            left: 0%;
            width: 30%;
        }


        45% {
            left: 70%;
            width: 30%;
        }

        50% {
            left: 100%;
            width: 0%;
        }

        55% {
            left: 70%;
            width: 30%;
        }

        95% {
            left: 0%;
            width: 30%;
        }

        100% {
            left: 0%;
            width: 0%;
        }

    }

    @media screen and (max-width: 600px) {
        #video {
            min-height: 375px;
            min-width: 500px;
            object-fit: cover;
        }

        body {
            background-color: #343a40 !important;
        }
    }
</style>
<video id="video" autoplay muted></video>
<div id="outcome">
    <h1> Results </h1>
    <div id="results"></div>
    <div id="classification"></div>
</div>

<div id="container_modal">
    <div id="container_loadbar">
        <div id="progress">
            <div id="message">Downloading...</div>
            <div id="progress-bar" class="progress-bar progress-bar-striped progress-bar-animated bg-info indeterminate
                role=" progressbar" aria-valuemin="0" aria-valuemax="100">
            </div>
        </div>
    </div>
</div>


<button id="toggleCamera" onclick="restart()">Cam</button>
<script>
    // Modelo de TensorFlow.js
    change_message("Loading Face Detector")
    show_loadbar()
    var currentStream
    var cameraSelector = true
    var model;
    var classification = {
        0: 'with_mask',
        1: 'without_mask',
        2: 'wrong_mask'
    }
    var colors = {
        0: 'green',
        1: 'red',
        2: 'blue'
    }
    const video = document.getElementById('video');
    var displaySize;
    var predictions = []

    navigator.getMedia = (navigator.getUserMedia ||
        navigator.webkitGetUserMedia ||
        navigator.mozGetUserMedia);
    /*
        Carga la red de detección posteriormente inicializa el vídeo a través de startVideo()
        Activar la barra de carga
    */
    Promise.all([
        faceapi.nets.ssdMobilenetv1.loadFromUri("{{ url_for('static', filename='models/') }}")
    ]).then(loadModel)

    /*
        Inicializa el vídeo si está la cámara activa y lo añade como source al elemento <video> del DOM
    */

    async function loadModel() {
        change_message("Loading Model...")

        model = await tf.loadLayersModel("{{ url_for('static', filename='models/tfjs/model.json') }}")
        // console.log(model.summary())

        startVideo()
    }



    function restart() {
        currentStream.getTracks().forEach(t => t.stop())
        start()
    }


    function startVideo() {


        if (cameraSelector) {
            cameraSelector = false
            const constraints = {
                video: {}
            }

        } else {
            cameraSelector = true

            const constraints = {
                video: {
                    facingMode: {
                        ideal: "environment"
                    }
                }
            }
        }

        navigator.getMedia(constraints,
            stream => video.srcObject = currentStream = stream ,
            err => console.error(err)
        )
    }

    video.addEventListener('play', async () => {
        const canvas = faceapi.createCanvasFromMedia(video)
        const ctx = canvas.getContext('2d')
        document.body.append(canvas)
        // Deberia actualizarse si cambia el tamaño de la pantalla
        const displaySize = {
            width: parseInt(window.getComputedStyle(video).width),
            height: parseInt(window.getComputedStyle(video).height)
        }
        faceapi.matchDimensions(canvas, displaySize)
        //if(mobile)
        const ssdMobilenetOptions = new faceapi.SsdMobilenetv1Options({
            minConfidence: 0.20,
            maxFaces: 20
        })
        // Hide loadbar
        change_message("Complete")
        hide_loadbar()

        setInterval(async () => {

            const detections = await faceapi.detectAllFaces(video, ssdMobilenetOptions)
            const resizedDetections = faceapi.resizeResults(detections, displaySize)

            ctx.clearRect(0, 0, canvas.width, canvas.height)
            // await extractFaceFromBox(video, resizedDetections)
            resizedDetections.forEach(async (d, i) => {
                await extractFaceFromBox(video, d, i)
                const drawBox = new faceapi.draw.DrawBox(d._box, {
                    boxColor: colors[predictions[i]],
                    label: classification[predictions[i]]
                })
                drawBox.draw(canvas)
            })
        }, 400)
    })


    // async function extractFaceFromBox(inputImage, detections) { 

    //     const faceImages = await faceapi.extractFaces(inputImage, detections)
    //     faceImages.forEach(async (img, i) => {
    //         const result = await model.predict(preprocess(img))
    //         const data = result.dataSync()
    //         predictions[i] = data.indexOf(Math.max(...data))
    //     })
    // }

    async function extractFaceFromBox(inputImage, detection, ind) {

        const faceImage = await faceapi.extractFaces(inputImage, [detection])
        const result = await model.predict(preprocess(faceImage[0]))
        const data = result.dataSync()
        predictions[ind] = data.indexOf(Math.max(...data))

    }

    function preprocess(img) {
        const image = tf.browser.fromPixels(img);
        const resized = tf.image.resizeBilinear(image, [96, 96]).toFloat()
        const normalized = tf.div(resized, tf.scalar(255.0)) //tf.scalar(1.0).sub();
        return normalized.expandDims(0)
    }



    // loadbar

    function show_loadbar() {
        document.querySelector('#container_modal').style.display = 'flex'
    }

    function hide_loadbar() {
        document.querySelector('#container_modal').style.display = 'none'
    }


    function change_message(message = "Downloading...") {
        var mesg = document.getElementById("message");
        mesg.textContent = message
    }
</script>
{% endblock %}